{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPg97IcxguIz92F5lThPu4W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saat12d/NaturalLanguageProcessing/blob/main/generative_deep_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Generation"
      ],
      "metadata": {
        "id": "y0ZezO2Vw2D_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGHq9iU_tSdV",
        "outputId": "3543971b-2261-4e26-dcb5-44ca3b5cfa8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-03 06:11:56--  https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M  16.4MB/s    in 8.2s    \n",
            "\n",
            "2023-08-03 06:12:05 (9.75 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a dataset from text files"
      ],
      "metadata": {
        "id": "xq6VcOT6ww1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "dataset = keras.utils.text_dataset_from_directory(directory = 'aclImdb', label_mode = None, batch_size = 256)\n",
        "dataset = dataset.map(lambda x: tf.strings.regex_replace(x, \"<br />\", \" \"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITXwqqz0wxGe",
        "outputId": "def096fb-6dc2-441a-c86e-1de2918ca36f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 100006 files belonging to 1 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing a TextVectorization layer"
      ],
      "metadata": {
        "id": "-4u0Q-4Q7uoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "sequence_length = 100\n",
        "vocab_size = 15000\n",
        "text_vectorization = TextVectorization(\n",
        "    max_tokens = vocab_size,\n",
        "    output_mode = 'int',\n",
        "    output_sequence_length = sequence_length\n",
        ")\n",
        "text_vectorization.adapt(dataset)"
      ],
      "metadata": {
        "id": "p57fhDqr7wdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up a language modelling dataset - where inputs are vectorized and target samples are same texts offset by one word"
      ],
      "metadata": {
        "id": "jkKrKe1L8mDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_lm_dataset(text_batch):\n",
        "  vectorized_sequences = text_vectorization(text_batch)\n",
        "  x = vectorized_sequences[:, :-1]\n",
        "  y = vectorized_sequences[:, 1:]\n",
        "  return x, y\n",
        "\n",
        "lm_dataset = dataset.map(prepare_lm_dataset, num_parallel_calls = 4)"
      ],
      "metadata": {
        "id": "CTuaC4e78wHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple Transformer based language model"
      ],
      "metadata": {
        "id": "XCMgjQkK-gcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "\n",
        "  def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.token_embeddings = layers.Embedding(input_dim = input_dim, output_dim = output_dim)\n",
        "    self.position_embeddings = layers.Embedding(input_dim = sequence_length, output_dim = output_dim)\n",
        "    self.sequence_length = sequence_length\n",
        "    self.input_dim = input_dim\n",
        "    self.output_dim = output_dim\n",
        "\n",
        "  def call(self, inputs):\n",
        "    length = tf.shape(inputs)[-1]\n",
        "    positions = tf.range(start = 0, limit = length, delta = 1)\n",
        "    embedded_tokens = self.token_embeddings(inputs)\n",
        "    embedded_positions = self.position_embeddings(positions)\n",
        "    return embedded_tokens + embedded_positions\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update({\n",
        "        \"output_dim\": self.output_dim,\n",
        "        \"sequence_length\": self.sequence_length,\n",
        "        \"input_dim\": self.input_dim\n",
        "    })\n",
        "    return config"
      ],
      "metadata": {
        "id": "y_rP0h3l-iTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(layers.Layer):\n",
        "  def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.embed_dim = embed_dim\n",
        "    self.dense_dim = dense_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.attention_1 = layers.MultiHeadAttention(num_heads = num_heads, key_dim = embed_dim)\n",
        "    self.attention_2 = layers.MultiHeadAttention(num_heads = num_heads, key_dim = embed_dim)\n",
        "    self.dense_proj = keras.Sequential([\n",
        "        layers.Dense(dense_dim, activation = 'relu'),\n",
        "        layers.Dense(embed_dim)\n",
        "    ])\n",
        "    self.layernorm_1 = layers.LayerNormalization()\n",
        "    self.layernorm_2 = layers.LayerNormalization()\n",
        "    self.layernorm_3 = layers.LayerNormalization()\n",
        "    self.supports_masking = True\n",
        "\n",
        "  def call(self, inputs, encoder_outputs, mask = None):\n",
        "    causal_mask = self.get_causal_attention_mask(inputs)\n",
        "    padding_mask = None\n",
        "    if mask is not None:\n",
        "      padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "      padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "    attention_output_1 = self.attention_1(query = inputs, value = inputs, key = inputs, attention_mask = causal_mask)\n",
        "    attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "    attention_output_2 = self.attention_2(query = attention_output_1, value = encoder_outputs, key = encoder_outputs, attention_mask = padding_mask)\n",
        "    attention_output_2 = self.layernorm_2(attention_output_1 + attention_output_2)\n",
        "    proj_output = self.dense_proj(attention_output_2)\n",
        "    return self.layernorm_3(attention_output_2 + proj_output)\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update({\n",
        "        \"embed_dim\": self.embed_dim,\n",
        "        \"num_heads\": self.num_heads,\n",
        "        \"dense_dim\": self.dense_dim\n",
        "    })\n",
        "    return config\n",
        "\n",
        "  def get_causal_attention_mask(self, inputs):\n",
        "    input_shape = tf.shape(inputs)\n",
        "    batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "    i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "    j = tf.range(sequence_length)\n",
        "    mask = tf.cast(i >= j, dtype = 'int32')\n",
        "    mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "    mult = tf.concat([tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype = tf.int32)], axis = 0)\n",
        "    return tf.tile(mask, mult)"
      ],
      "metadata": {
        "id": "rcenqjzh-olK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 256\n",
        "latent_dim = 2048\n",
        "num_heads = 2\n",
        "\n",
        "inputs = keras.Input(shape = (None,), dtype = 'int64')\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, x)\n",
        "outputs = layers.Dense(vocab_size, activation = 'softmax')(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer = 'rmsprop', loss = 'sparse_categorical_crossentropy')"
      ],
      "metadata": {
        "id": "dCf7VvEU-szC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Text Generation callback"
      ],
      "metadata": {
        "id": "4FQ9_uJP_VPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "tokens_index = dict(enumerate(text_vectorization.get_vocabulary()))\n",
        "\n",
        "def sample_next(predictions, temperature = 1.0):\n",
        "  predictions = np.asarray(predictions).astype('float64')\n",
        "  predictions = np.log(predictions) / temperature\n",
        "  exp_preds = np.exp(predictions)\n",
        "  predictions = exp_preds / np.sum(exp_preds)\n",
        "  probas = np.random.multinomial(1, predictions, 1)\n",
        "  return np.argmax(probas)\n",
        "\n",
        "class TextGenerator(keras.callbacks.Callback):\n",
        "  def __init__(self, prompt, generate_length, model_input_length, temperatures = (1.,), print_freq = 1):\n",
        "    self.prompt = prompt\n",
        "    self.generate_length = generate_length\n",
        "    self.model_input_length = model_input_length\n",
        "    self.temperatures = temperatures\n",
        "    self.print_freq = print_freq\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs = None):\n",
        "    if epoch + 1 % self.print_freq != 0:\n",
        "      return\n",
        "    for temperature in self.temperatures:\n",
        "      print(\"== Generating with temperature \", temperature)\n",
        "      sentence = self.prompt\n",
        "      for i in range(self.generate_length):\n",
        "        tokenized_sentence = text_vectorization([sentence])\n",
        "        predictions = self.model(tokenized_sentence)\n",
        "        next_token = sample_next(predictions[0, i, :])\n",
        "        sampled_token = tokens_index[next_token]\n",
        "        sentence += \" \" + sampled_token\n",
        "      print(sentence)\n",
        "\n",
        "prompt = 'This movie'\n",
        "text_gen_callback = TextGenerator(prompt, generate_length = 50, model_input_length = sequence_length, temperatures=(0.2, 0.5, 0.7, 1., 1.5))\n"
      ],
      "metadata": {
        "id": "go6YZrOh_Wna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(lm_dataset, epochs = 200, callbacks = [text_gen_callback])"
      ],
      "metadata": {
        "id": "m67AXuGLBZvW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}