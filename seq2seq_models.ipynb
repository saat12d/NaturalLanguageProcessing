{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPvM557vqB0KL2nf2kZwl+t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saat12d/NaturalLanguageProcessing/blob/main/seq2seq_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Translation"
      ],
      "metadata": {
        "id": "1Ih7PLDVu93N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "FwsrZfbftfap"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Z7p4cfUu0br",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a091a5e-8813-40b9-9e0a-f94bb67f913f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-02 06:41:20--  http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.182.128, 64.233.183.128, 173.194.193.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.182.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2638744 (2.5M) [application/zip]\n",
            "Saving to: ‘spa-eng.zip’\n",
            "\n",
            "\rspa-eng.zip           0%[                    ]       0  --.-KB/s               \rspa-eng.zip         100%[===================>]   2.52M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-08-02 06:41:20 (117 MB/s) - ‘spa-eng.zip’ saved [2638744/2638744]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
        "!unzip -q spa-eng.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parsing data"
      ],
      "metadata": {
        "id": "Ykvi0Km_vFdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_file = 'spa-eng/spa.txt'\n",
        "with open(text_file) as f:\n",
        "  lines = f.read().split('\\n')[:-1]\n",
        "text_pairs = []\n",
        "\n",
        "for line in lines:\n",
        "  english, spanish = line.split('\\t')\n",
        "  spanish = '[start]' + spanish + '[end]'\n",
        "  text_pairs.append((english, spanish))\n",
        "\n",
        "import random\n",
        "print(random.choice(text_pairs))"
      ],
      "metadata": {
        "id": "Y2lPpwIUvGfM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "199f9b6d-869e-4abe-a267-99811000ea56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('I need your advice.', '[start]Necesito tu consejo.[end]')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating training, validation, and test sets"
      ],
      "metadata": {
        "id": "R6iJpURRvxk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples: num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples:]"
      ],
      "metadata": {
        "id": "IzoqM4Lgv0_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorizing the English and Spanish text pairs"
      ],
      "metadata": {
        "id": "VXWkx_VKwU7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import re\n",
        "import string\n",
        "\n",
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "  lowercase = tf.strings.lower(input_string)\n",
        "  return tf.strings.regex_replace(lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
        "\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "\n",
        "source_vectorization = layers.TextVectorization(\n",
        "    max_tokens = vocab_size,\n",
        "    output_mode = 'int',\n",
        "    output_sequence_length = sequence_length\n",
        ")\n",
        "\n",
        "target_vectorization = layers.TextVectorization(\n",
        "    max_tokens = vocab_size,\n",
        "    output_mode = 'int',\n",
        "    output_sequence_length = sequence_length + 1,\n",
        "    standardize = custom_standardization\n",
        ")\n",
        "\n",
        "train_english_texts = [pair[0] for pair in train_pairs]\n",
        "train_spanish_texts = [pair[1] for pair in train_pairs]\n",
        "source_vectorization.adapt(train_english_texts)\n",
        "target_vectorization.adapt(train_spanish_texts)"
      ],
      "metadata": {
        "id": "agQzPkHHovhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing Datasets for translation task"
      ],
      "metadata": {
        "id": "V8nND_Kwrdtf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "def format_dataset(eng, spa):\n",
        "  eng = source_vectorization(eng)\n",
        "  spa = target_vectorization(spa)\n",
        "  return ({\n",
        "      \"english\": eng,\n",
        "      # The input Spanish sentence doesn’t include the last token to keep inputs and targets at the same length.\n",
        "      \"spanish\": spa[:, :-1]\n",
        "  }, spa[:, 1:]) # The target Spanish sentence is one step ahead. Both are still the same length (20 words).\n",
        "\n",
        "def make_dataset(pairs):\n",
        "  eng_texts, spa_texts = zip(*pairs)\n",
        "  eng_texts = list(eng_texts)\n",
        "  spa_texts = list(spa_texts)\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  dataset = dataset.map(format_dataset, num_parallel_calls = 4)\n",
        "  return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ],
      "metadata": {
        "id": "bgo3zQbErfwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using RNN for seq2seq learning"
      ],
      "metadata": {
        "id": "_7nFhCNqtjVa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GRU based Encoder"
      ],
      "metadata": {
        "id": "TBMnAsI-uCOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 256\n",
        "latent_dim = 1024\n",
        "\n",
        "source = keras.Input(shape = (None,), dtype = 'int64', name = 'english')\n",
        "x = layers.Embedding(vocab_size, embed_dim, mask_zero = True)(source)\n",
        "encoded_source = layers.Bidirectional(layers.GRU(latent_dim), merge_mode = 'sum')(x)"
      ],
      "metadata": {
        "id": "4Jv_N-dKtlxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GRU based decoder and end-to-end model"
      ],
      "metadata": {
        "id": "W-LudiiMuxl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "past_target = keras.Input(shape = (None,), dtype = 'int64', name = 'spanish')\n",
        "x = layers.Embedding(vocab_size, embed_dim, mask_zero = True)(past_target)\n",
        "decoder_gru = layers.GRU(latent_dim, return_sequences = True)\n",
        "x = decoder_gru(x, initial_state = encoded_source)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "target_next_step = layers.Dense(vocab_size, activation = 'softmax')(x)\n",
        "seq2seq_rnn = keras.Model([source, past_target], target_next_step)"
      ],
      "metadata": {
        "id": "aUutSQKju03q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "sqKlUNIPvqSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq_rnn.compile(optimizer = 'rmsprop', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
        "seq2seq_rnn.fit(train_ds, epochs = 15, validation_data = val_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iruM1Bcuvq2N",
        "outputId": "e0fb360b-f745-47b4-815f-ae91426e5094"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "1302/1302 [==============================] - 146s 98ms/step - loss: 5.4497 - accuracy: 0.2324 - val_loss: 4.5663 - val_accuracy: 0.3044\n",
            "Epoch 2/15\n",
            "1302/1302 [==============================] - 106s 81ms/step - loss: 4.4346 - accuracy: 0.3281 - val_loss: 3.8969 - val_accuracy: 0.3851\n",
            "Epoch 3/15\n",
            "1302/1302 [==============================] - 106s 81ms/step - loss: 3.8720 - accuracy: 0.3911 - val_loss: 3.4509 - val_accuracy: 0.4437\n",
            "Epoch 4/15\n",
            "1302/1302 [==============================] - 106s 81ms/step - loss: 3.4524 - accuracy: 0.4392 - val_loss: 3.1498 - val_accuracy: 0.4837\n",
            "Epoch 5/15\n",
            "1302/1302 [==============================] - 106s 81ms/step - loss: 3.1198 - accuracy: 0.4785 - val_loss: 2.9543 - val_accuracy: 0.5126\n",
            "Epoch 6/15\n",
            "1302/1302 [==============================] - 106s 81ms/step - loss: 2.8540 - accuracy: 0.5105 - val_loss: 2.7837 - val_accuracy: 0.5376\n",
            "Epoch 7/15\n",
            "1302/1302 [==============================] - 106s 81ms/step - loss: 2.6242 - accuracy: 0.5387 - val_loss: 2.6632 - val_accuracy: 0.5540\n",
            "Epoch 8/15\n",
            "1302/1302 [==============================] - 106s 81ms/step - loss: 2.4265 - accuracy: 0.5633 - val_loss: 2.5707 - val_accuracy: 0.5677\n",
            "Epoch 9/15\n",
            "1302/1302 [==============================] - 106s 81ms/step - loss: 2.2639 - accuracy: 0.5847 - val_loss: 2.5061 - val_accuracy: 0.5763\n",
            "Epoch 10/15\n",
            "1302/1302 [==============================] - 105s 81ms/step - loss: 2.1167 - accuracy: 0.6028 - val_loss: 2.4464 - val_accuracy: 0.5860\n",
            "Epoch 11/15\n",
            "1302/1302 [==============================] - 106s 81ms/step - loss: 1.9945 - accuracy: 0.6198 - val_loss: 2.4175 - val_accuracy: 0.5902\n",
            "Epoch 12/15\n",
            "1302/1302 [==============================] - 106s 81ms/step - loss: 1.8764 - accuracy: 0.6352 - val_loss: 2.3746 - val_accuracy: 0.5971\n",
            "Epoch 13/15\n",
            "1302/1302 [==============================] - 105s 81ms/step - loss: 1.7834 - accuracy: 0.6485 - val_loss: 2.3463 - val_accuracy: 0.6012\n",
            "Epoch 14/15\n",
            "1302/1302 [==============================] - 105s 81ms/step - loss: 1.7005 - accuracy: 0.6592 - val_loss: 2.3226 - val_accuracy: 0.6042\n",
            "Epoch 15/15\n",
            "1302/1302 [==============================] - 105s 81ms/step - loss: 1.6226 - accuracy: 0.6713 - val_loss: 2.3084 - val_accuracy: 0.6066\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x792bb4375450>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Translating new sentences"
      ],
      "metadata": {
        "id": "2PYKg8RGC6m8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "spa_vocab = target_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "  tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "  decoded_sentence = '[start]'\n",
        "  for i in range(max_decoded_sentence_length):\n",
        "    tokenized_target_sentence = target_vectorization([decoded_sentence])\n",
        "    next_token_predictions = seq2seq_rnn.predict([tokenized_input_sentence, tokenized_target_sentence])\n",
        "    sampled_token_index = np.argmax(next_token_predictions[0, i, :])\n",
        "    sampled_token = spa_index_lookup[sampled_token_index]\n",
        "    decoded_sentence += \" \" + sampled_token\n",
        "    if sampled_token == \"[end]\":\n",
        "      break\n",
        "  return decoded_sentence\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(5):\n",
        "  input_sentence = random.choice(test_eng_texts)\n",
        "  print(\"-\")\n",
        "  print(input_sentence)\n",
        "  print(decode_sequence(input_sentence))"
      ],
      "metadata": {
        "id": "0IUXg5UlC8cL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer for Machine Translation"
      ],
      "metadata": {
        "id": "pt_JQiIR6R11"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer Encoder"
      ],
      "metadata": {
        "id": "BTVWs4h-6WHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "  def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.embed_dim = embed_dim\n",
        "    self.dense_dim = dense_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.attention = layers.MultiHeadAttention(num_heads = num_heads, key_dim = embed_dim)\n",
        "    self.dense_proj = keras.Sequential([layers.Dense(dense_dim, activation = 'relu'), layers.Dense(embed_dim)])\n",
        "    self.layernorm_1 = layers.LayerNormalization()\n",
        "    self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "  def call(self, inputs, mask = None):\n",
        "    if mask is not None:\n",
        "      mask = mask[:, tf.newaxis, :]\n",
        "    attention_output = self.attention(inputs, inputs, attention_mask = mask)\n",
        "    proj_input = self.layernorm_1(inputs + attention_output)\n",
        "    proj_output = self.dense_proj(proj_input)\n",
        "    return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update({\n",
        "        \"embed_dim\": self.embed_dim,\n",
        "        \"num_heads\": self.num_heads,\n",
        "        \"dense_dim\": self.dense_dim\n",
        "    })\n",
        "    return config"
      ],
      "metadata": {
        "id": "soWjLk316XQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer Decoder"
      ],
      "metadata": {
        "id": "-1mgE2c86bpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(layers.Layer):\n",
        "  def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.embed_dim = embed_dim\n",
        "    self.dense_dim = dense_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.attention_1 = layers.MultiHeadAttention(num_heads = num_heads, key_dim = embed_dim)\n",
        "    self.attention_2 = layers.MultiHeadAttention(num_heads = num_heads, key_dim = embed_dim)\n",
        "    self.dense_proj = keras.Sequential([\n",
        "        layers.Dense(dense_dim, activation = 'relu'),\n",
        "        layers.Dense(embed_dim)\n",
        "    ])\n",
        "    self.layernorm_1 = layers.LayerNormalization()\n",
        "    self.layernorm_2 = layers.LayerNormalization()\n",
        "    self.layernorm_3 = layers.LayerNormalization()\n",
        "    self.supports_masking = True\n",
        "\n",
        "  def call(self, inputs, encoder_outputs, mask = None):\n",
        "    causal_mask = self.get_causal_attention_mask(inputs)\n",
        "    padding_mask = None\n",
        "    if mask is not None:\n",
        "      padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "      padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "    attention_output_1 = self.attention_1(query = inputs, value = inputs, key = inputs, attention_mask = causal_mask)\n",
        "    attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "    attention_output_2 = self.attention_2(query = attention_output_1, value = encoder_outputs, key = encoder_outputs, attention_mask = padding_mask)\n",
        "    attention_output_2 = self.layernorm_2(attention_output_1 + attention_output_2)\n",
        "    proj_output = self.dense_proj(attention_output_2)\n",
        "    return self.layernorm_3(attention_output_2 + proj_output)\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update({\n",
        "        \"embed_dim\": self.embed_dim,\n",
        "        \"num_heads\": self.num_heads,\n",
        "        \"dense_dim\": self.dense_dim\n",
        "    })\n",
        "    return config\n",
        "\n",
        "  def get_causal_attention_mask(self, inputs):\n",
        "    input_shape = tf.shape(inputs)\n",
        "    batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "    i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "    j = tf.range(sequence_length)\n",
        "    mask = tf.cast(i >= j, dtype = 'int32')\n",
        "    mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "    mult = tf.concat([tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype = tf.int32)], axis = 0)\n",
        "    return tf.tile(mask, mult)\n",
        "\n"
      ],
      "metadata": {
        "id": "78lBG2XL6c-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positional Embedding"
      ],
      "metadata": {
        "id": "q3X_yFXM-CK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "\n",
        "  def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.token_embeddings = layers.Embedding(input_dim = input_dim, output_dim = output_dim)\n",
        "    self.position_embeddings = layers.Embedding(input_dim = sequence_length, output_dim = output_dim)\n",
        "    self.sequence_length = sequence_length\n",
        "    self.input_dim = input_dim\n",
        "    self.output_dim = output_dim\n",
        "\n",
        "  def call(self, inputs):\n",
        "    length = tf.shape(inputs)[-1]\n",
        "    positions = tf.range(start = 0, limit = length, delta = 1)\n",
        "    embedded_tokens = self.token_embeddings(inputs)\n",
        "    embedded_positions = self.position_embeddings(positions)\n",
        "    return embedded_tokens + embedded_positions\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update({\n",
        "        \"output_dim\": self.output_dim,\n",
        "        \"sequence_length\": self.sequence_length,\n",
        "        \"input_dim\": self.input_dim\n",
        "    })\n",
        "    return config"
      ],
      "metadata": {
        "id": "nl6Tzqv--DVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "End-to-end Transformer model"
      ],
      "metadata": {
        "id": "rCdTcCed-GLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 256\n",
        "dense_dim = 2048\n",
        "num_heads = 8\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "\n",
        "encoder_inputs = keras.Input(shape = (None,), dtype = 'int64', name = 'english')\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "\n",
        "decoder_inputs = keras.Input(shape = (None,), dtype = 'int64', name = 'spanish')\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation = 'softmax')(x)\n",
        "\n",
        "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "metadata": {
        "id": "2-avMa4P-JA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the transformer"
      ],
      "metadata": {
        "id": "WqSANRQgEKe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "transformer.fit(train_ds, epochs=30, validation_data=val_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLK5stLSEJZs",
        "outputId": "fe3cf7b1-de99-4a4b-cbeb-fd5b61d8263f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "1302/1302 [==============================] - 118s 81ms/step - loss: 1.8423 - accuracy: 0.7582 - val_loss: 1.5614 - val_accuracy: 0.7744\n",
            "Epoch 2/30\n",
            "1302/1302 [==============================] - 88s 68ms/step - loss: 1.4814 - accuracy: 0.7845 - val_loss: 1.2676 - val_accuracy: 0.7968\n",
            "Epoch 3/30\n",
            "1302/1302 [==============================] - 88s 68ms/step - loss: 1.2800 - accuracy: 0.7986 - val_loss: 1.1492 - val_accuracy: 0.8051\n",
            "Epoch 4/30\n",
            "1302/1302 [==============================] - 88s 68ms/step - loss: 1.1769 - accuracy: 0.8068 - val_loss: 1.0792 - val_accuracy: 0.8121\n",
            "Epoch 5/30\n",
            "1302/1302 [==============================] - 88s 68ms/step - loss: 1.1159 - accuracy: 0.8126 - val_loss: 1.0494 - val_accuracy: 0.8169\n",
            "Epoch 6/30\n",
            "1302/1302 [==============================] - 88s 68ms/step - loss: 1.0743 - accuracy: 0.8169 - val_loss: 1.0452 - val_accuracy: 0.8188\n",
            "Epoch 7/30\n",
            "1302/1302 [==============================] - 90s 69ms/step - loss: 1.0443 - accuracy: 0.8210 - val_loss: 1.0368 - val_accuracy: 0.8200\n",
            "Epoch 8/30\n",
            "1302/1302 [==============================] - 88s 68ms/step - loss: 1.0213 - accuracy: 0.8245 - val_loss: 1.0423 - val_accuracy: 0.8194\n",
            "Epoch 9/30\n",
            "1302/1302 [==============================] - 88s 68ms/step - loss: 1.0024 - accuracy: 0.8279 - val_loss: 1.0558 - val_accuracy: 0.8198\n",
            "Epoch 10/30\n",
            "1302/1302 [==============================] - 88s 68ms/step - loss: 0.9859 - accuracy: 0.8311 - val_loss: 1.0654 - val_accuracy: 0.8201\n",
            "Epoch 11/30\n",
            "1302/1302 [==============================] - 88s 68ms/step - loss: 0.9724 - accuracy: 0.8336 - val_loss: 1.0778 - val_accuracy: 0.8200\n",
            "Epoch 12/30\n",
            "1302/1302 [==============================] - 88s 68ms/step - loss: 0.9591 - accuracy: 0.8364 - val_loss: 1.0885 - val_accuracy: 0.8207\n",
            "Epoch 13/30\n",
            "1302/1302 [==============================] - 88s 68ms/step - loss: 0.9460 - accuracy: 0.8387 - val_loss: 1.1242 - val_accuracy: 0.8116\n",
            "Epoch 14/30\n",
            "1302/1302 [==============================] - 88s 68ms/step - loss: 0.9337 - accuracy: 0.8414 - val_loss: 1.1228 - val_accuracy: 0.8208\n",
            "Epoch 15/30\n",
            "1302/1302 [==============================] - 88s 68ms/step - loss: 0.9209 - accuracy: 0.8436 - val_loss: 1.1238 - val_accuracy: 0.8163\n",
            "Epoch 16/30\n",
            "1302/1302 [==============================] - 88s 68ms/step - loss: 0.9086 - accuracy: 0.8459 - val_loss: 1.1641 - val_accuracy: 0.8095\n",
            "Epoch 17/30\n",
            "1302/1302 [==============================] - 88s 68ms/step - loss: 0.8980 - accuracy: 0.8476 - val_loss: 1.1420 - val_accuracy: 0.8167\n",
            "Epoch 18/30\n",
            "1302/1302 [==============================] - 88s 68ms/step - loss: 0.8853 - accuracy: 0.8501 - val_loss: 1.1733 - val_accuracy: 0.8112\n",
            "Epoch 19/30\n",
            "1302/1302 [==============================] - 88s 68ms/step - loss: 0.8746 - accuracy: 0.8521 - val_loss: 1.1673 - val_accuracy: 0.8171\n",
            "Epoch 20/30\n",
            "1302/1302 [==============================] - 88s 68ms/step - loss: 0.8653 - accuracy: 0.8540 - val_loss: 1.1705 - val_accuracy: 0.8165\n",
            "Epoch 21/30\n",
            "1302/1302 [==============================] - 88s 68ms/step - loss: 0.8539 - accuracy: 0.8559 - val_loss: 1.1777 - val_accuracy: 0.8160\n",
            "Epoch 22/30\n",
            "1302/1302 [==============================] - 88s 68ms/step - loss: 0.8447 - accuracy: 0.8577 - val_loss: 1.1929 - val_accuracy: 0.8134\n",
            "Epoch 23/30\n",
            "1302/1302 [==============================] - 90s 69ms/step - loss: 0.8368 - accuracy: 0.8593 - val_loss: 1.2026 - val_accuracy: 0.8171\n",
            "Epoch 24/30\n",
            "1302/1302 [==============================] - 92s 71ms/step - loss: 0.8290 - accuracy: 0.8605 - val_loss: 1.2102 - val_accuracy: 0.8128\n",
            "Epoch 25/30\n",
            "1302/1302 [==============================] - 90s 69ms/step - loss: 0.8189 - accuracy: 0.8626 - val_loss: 1.2172 - val_accuracy: 0.8128\n",
            "Epoch 26/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 0.8106 - accuracy: 0.8641 - val_loss: 1.2301 - val_accuracy: 0.8162\n",
            "Epoch 27/30\n",
            "1302/1302 [==============================] - 88s 68ms/step - loss: 0.8042 - accuracy: 0.8653 - val_loss: 1.2479 - val_accuracy: 0.8164\n",
            "Epoch 28/30\n",
            "1302/1302 [==============================] - 90s 69ms/step - loss: 0.7959 - accuracy: 0.8672 - val_loss: 1.2502 - val_accuracy: 0.8147\n",
            "Epoch 29/30\n",
            "1302/1302 [==============================] - 88s 68ms/step - loss: 0.7891 - accuracy: 0.8685 - val_loss: 1.2702 - val_accuracy: 0.8145\n",
            "Epoch 30/30\n",
            "1302/1302 [==============================] - 90s 69ms/step - loss: 0.7834 - accuracy: 0.8695 - val_loss: 1.2716 - val_accuracy: 0.8125\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7bcf01870760>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Translating new sentences with Transformer Model"
      ],
      "metadata": {
        "id": "-Djpzb4HM-bw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "spa_vocab = target_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "  tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "  decoded_sentence = \"[start]\"\n",
        "  for i in range(max_decoded_sentence_length):\n",
        "    tokenized_target_sentence = target_vectorization([decoded_sentence])[:, :-1]\n",
        "    predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
        "    sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "    sampled_token = spa_index_lookup[sampled_token_index]\n",
        "    decoded_sentence += \" \" + sampled_token\n",
        "    if sampled_token == \"[end]\":\n",
        "      break\n",
        "  return decoded_sentence\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(5):\n",
        "  input_sentence = random.choice(test_eng_texts)\n",
        "  print(\"-\")\n",
        "  print(input_sentence)\n",
        "  print(decode_sequence(input_sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKSO2JrANAgN",
        "outputId": "cf31c5d3-2cf7-49bb-fa8b-91991bfbf1b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-\n",
            "Can we work on that now?\n",
            "[start] trabajar eso eso ese ahora[end]               \n",
            "-\n",
            "I need the key to decode this message.\n",
            "[start] la llave de es este               \n",
            "-\n",
            "Tom felt cold.\n",
            "[start] se sintió frío[end]                 \n",
            "-\n",
            "It wouldn't surprise me if Tom and Mary got married.\n",
            "[start] me sorpresa si si tom tom mary se            \n",
            "-\n",
            "What do you have against Tom?\n",
            "[start] qué [UNK] en                 \n"
          ]
        }
      ]
    }
  ]
}